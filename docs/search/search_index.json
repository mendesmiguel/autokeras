{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Auto-Keras is an open source software library for automated machine learning (AutoML). The ultimate goal of AutoML is to allow domain experts with limited data science or machine learning background easily accessible to deep learning models. Auto-Keras provides functions to automatically search for architecture and hyperparameters of deep learning models. Installation To install the package, please use the pip installation as follows: pip install autokeras Note: currently, Auto-Keras only supports Python 3.6. Example Here is a short example of using the package. import autokeras as ak clf = ak.ImageClassifier() clf.fit(x_train, y_train) results = clf.predict(x_test) Documentation For the documentation, please visit the Auto-Keras official website. Citing this work If you use Auto-Keras in a scientific publication, you are highly encouraged (though not required) to cite the following paper: Efficient Neural Architecture Search with Network Morphism. Haifeng Jin, Qingquan Song, and Xia Hu. arXiv:1806.10282 . Biblatex entry: @online{jin2018efficient, author = {Haifeng Jin and Qingquan Song and Xia Hu}, title = {Efficient Neural Architecture Search with Network Morphism}, date = {2018-06-27}, year = {2018}, eprintclass = {cs.LG}, eprinttype = {arXiv}, eprint = {cs.LG/1806.10282}, } DISCLAIMER Please note that this is a pre-release version of the Auto-Keras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \u201cas is\u201d and \u201cas available\u201d basis. Auto-Keras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. Auto-Keras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user\u2019s own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated.","title":"Home"},{"location":"#installation","text":"To install the package, please use the pip installation as follows: pip install autokeras Note: currently, Auto-Keras only supports Python 3.6.","title":"Installation"},{"location":"#example","text":"Here is a short example of using the package. import autokeras as ak clf = ak.ImageClassifier() clf.fit(x_train, y_train) results = clf.predict(x_test)","title":"Example"},{"location":"#documentation","text":"For the documentation, please visit the Auto-Keras official website.","title":"Documentation"},{"location":"#citing-this-work","text":"If you use Auto-Keras in a scientific publication, you are highly encouraged (though not required) to cite the following paper: Efficient Neural Architecture Search with Network Morphism. Haifeng Jin, Qingquan Song, and Xia Hu. arXiv:1806.10282 . Biblatex entry: @online{jin2018efficient, author = {Haifeng Jin and Qingquan Song and Xia Hu}, title = {Efficient Neural Architecture Search with Network Morphism}, date = {2018-06-27}, year = {2018}, eprintclass = {cs.LG}, eprinttype = {arXiv}, eprint = {cs.LG/1806.10282}, }","title":"Citing this work"},{"location":"#disclaimer","text":"Please note that this is a pre-release version of the Auto-Keras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \u201cas is\u201d and \u201cas available\u201d basis. Auto-Keras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. Auto-Keras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user\u2019s own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated.","title":"DISCLAIMER"},{"location":"about/","text":"About This package is developed by DATA LAB at Texas A M University.","title":"About"},{"location":"about/#about","text":"This package is developed by DATA LAB at Texas A M University.","title":"About"},{"location":"bayesian/","text":"","title":"Bayesian"},{"location":"classifier/","text":"_validate Check x_train's type and the shape of x_train, y_train. read_csv_file Read the cvs file and returns two seperate list containing images name and their labels Args csv_file_path : Path to the CVS file. Returns img_file_names list containing images names and img_label list containing their respective labels. read_images Reads the images from the path and return there numpy.ndarray instance Args img_file_names : List containing images names images_dir_path : Path to directory containing images load_image_dataset Load images from the files and labels from a csv file. Second, the dataset is a set of images and the labels are in a CSV file. The CSV file should contain two columns whose names are 'File Name' and 'Label'. The file names in the first column should match the file names of the images with extensions, e.g., .jpg, .png. The path to the CSV file should be passed through the csv_file_path. The path to the directory containing all the images should be passed through image_path. Args csv_file_path : CVS file path. images_path : Path where images exist. Returns x: Four dimensional numpy.ndarray. The channel dimension is the last dimension. y: The labels. ImageClassifier The image classifier class. It is used for image classification. It searches convolutional neural network architectures for the best configuration for the dataset. Attributes path : A path to the directory to save the classifier. y_encoder : An instance of OneHotEncoder for y_train (array of categorical labels). verbose : A boolean value indicating the verbosity mode. searcher : An instance of BayesianSearcher. It search different neural architecture to find the best model. searcher_args : A dictionary containing the parameters for the searcher's init function. augment : A boolean value indicating whether the data needs augmentation. init Initialize the instance. The classifier will be loaded from the files in 'path' if parameter 'resume' is True. Otherwise it would create a new one. Args verbose : An boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. resume : An boolean. If True, the classifier will continue to previous work saved in path. Otherwise, the classifier will start a new search. augment : A boolean value indicating whether the data needs augmentation. fit Find the best neural architecture and train it. Based on the given dataset, the function will find the best neural architecture for it. The dataset is in numpy.ndarray format. So they training data should be passed through x_train, y_train. Args x_train : An numpy.ndarray instance contains the training data. y_train : An numpy.ndarray instance contains the label of the training data. time_limit : The time limit for the search in seconds. predict Return predict result for the testing data. Args x_test : An instance of numpy.ndarray contains the testing data. Returns An numpy.ndarray containing the results. evaluate Return the accuracy score between predict value and test_y. final_fit Final training after found the best architecture. Args x_train : An numpy.ndarray of training data. y_train : An numpy.ndarray of training targets. x_test : An numpy.ndarray of testing data. y_test : An numpy.ndarray of testing targets. trainer_args : A dictionary containing the parameters of the ModelTrainer constructure. retrain : A boolean of whether reinitialize the weights of the model. get_best_model_id Returns: An integer. The best model id.","title":"classifier"},{"location":"classifier/#_validate","text":"Check x_train's type and the shape of x_train, y_train.","title":"_validate"},{"location":"classifier/#read_csv_file","text":"Read the cvs file and returns two seperate list containing images name and their labels","title":"read_csv_file"},{"location":"classifier/#args","text":"csv_file_path : Path to the CVS file.","title":"Args"},{"location":"classifier/#returns","text":"img_file_names list containing images names and img_label list containing their respective labels.","title":"Returns"},{"location":"classifier/#read_images","text":"Reads the images from the path and return there numpy.ndarray instance","title":"read_images"},{"location":"classifier/#args_1","text":"img_file_names : List containing images names images_dir_path : Path to directory containing images","title":"Args"},{"location":"classifier/#load_image_dataset","text":"Load images from the files and labels from a csv file. Second, the dataset is a set of images and the labels are in a CSV file. The CSV file should contain two columns whose names are 'File Name' and 'Label'. The file names in the first column should match the file names of the images with extensions, e.g., .jpg, .png. The path to the CSV file should be passed through the csv_file_path. The path to the directory containing all the images should be passed through image_path.","title":"load_image_dataset"},{"location":"classifier/#args_2","text":"csv_file_path : CVS file path. images_path : Path where images exist.","title":"Args"},{"location":"classifier/#returns_1","text":"x: Four dimensional numpy.ndarray. The channel dimension is the last dimension. y: The labels.","title":"Returns"},{"location":"classifier/#imageclassifier","text":"The image classifier class. It is used for image classification. It searches convolutional neural network architectures for the best configuration for the dataset.","title":"ImageClassifier"},{"location":"classifier/#attributes","text":"path : A path to the directory to save the classifier. y_encoder : An instance of OneHotEncoder for y_train (array of categorical labels). verbose : A boolean value indicating the verbosity mode. searcher : An instance of BayesianSearcher. It search different neural architecture to find the best model. searcher_args : A dictionary containing the parameters for the searcher's init function. augment : A boolean value indicating whether the data needs augmentation.","title":"Attributes"},{"location":"classifier/#init","text":"Initialize the instance. The classifier will be loaded from the files in 'path' if parameter 'resume' is True. Otherwise it would create a new one.","title":"init"},{"location":"classifier/#args_3","text":"verbose : An boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. resume : An boolean. If True, the classifier will continue to previous work saved in path. Otherwise, the classifier will start a new search. augment : A boolean value indicating whether the data needs augmentation.","title":"Args"},{"location":"classifier/#fit","text":"Find the best neural architecture and train it. Based on the given dataset, the function will find the best neural architecture for it. The dataset is in numpy.ndarray format. So they training data should be passed through x_train, y_train.","title":"fit"},{"location":"classifier/#args_4","text":"x_train : An numpy.ndarray instance contains the training data. y_train : An numpy.ndarray instance contains the label of the training data. time_limit : The time limit for the search in seconds.","title":"Args"},{"location":"classifier/#predict","text":"Return predict result for the testing data.","title":"predict"},{"location":"classifier/#args_5","text":"x_test : An instance of numpy.ndarray contains the testing data.","title":"Args"},{"location":"classifier/#returns_2","text":"An numpy.ndarray containing the results.","title":"Returns"},{"location":"classifier/#evaluate","text":"Return the accuracy score between predict value and test_y.","title":"evaluate"},{"location":"classifier/#final_fit","text":"Final training after found the best architecture.","title":"final_fit"},{"location":"classifier/#args_6","text":"x_train : An numpy.ndarray of training data. y_train : An numpy.ndarray of training targets. x_test : An numpy.ndarray of testing data. y_test : An numpy.ndarray of testing targets. trainer_args : A dictionary containing the parameters of the ModelTrainer constructure. retrain : A boolean of whether reinitialize the weights of the model.","title":"Args"},{"location":"classifier/#get_best_model_id","text":"Returns: An integer. The best model id.","title":"get_best_model_id"},{"location":"constant/","text":"","title":"Constant"},{"location":"generator/","text":"","title":"generator"},{"location":"graph/","text":"Graph A class represent the neural architecture graph of a Keras model. Graph extracts the neural architecture graph from a Keras model. Each node in the graph is a intermediate tensor between layers. Each layer is an edge in the graph. Notably, multiple edges may refer to the same layer. (e.g. Add layer is adding two tensor into one tensor. So it is related to two edges.) Attributes weighted : A boolean of whether the weights and biases in the neural network should be included in the graph. input_shape : Tuple of integers, does not include the batch axis. node_list : A list of integers, the indices of the list are the identifiers. layer_list : A list of stub layers, the indices of the list are the identifiers. node_to_id : A dict instance mapping from node integers to their identifiers. layer_to_id : A dict instance mapping from stub layers to their identifiers. layer_id_to_input_node_ids : A dict instance mapping from layer identifiers to their input nodes identifiers. adj_list : A two dimensional list. The adjacency list of the graph. The first dimension is identified by tensor identifiers. In each edge list, the elements are two-element tuples of (tensor identifier, layer identifier). reverse_adj_list : A reverse adjacent list in the same format as adj_list. operation_history : A list saving all the network morphism operations. vis : A dictionary of temporary storage for whether an local operation has been done during the network morphism. n_nodes Return the number of nodes in the model. n_layers Return the number of layers in the model. _add_node Add node to node list if it not in node list. _add_edge Add edge to the graph. _redirect_edge Redirect the edge to a new node. Change the edge originally from u_id to v_id into an edge from u_id to new_v_id while keeping all other property of the edge the same. _replace_layer Replace the layer with a new layer. topological_order Return the topological order of the node ids. _search Search the graph for widening the layers. Args u : The starting node identifier. start_dim : The position to insert the additional dimensions. total_dim : The total number of dimensions the layer has before widening. n_add : The number of dimensions to add. to_conv_deeper_model Insert a relu-conv-bn block after the target block. Args target_id : A convolutional layer ID. The new block should be inserted after the block. kernel_size : An integer. The kernel size of the new convolutional layer. to_wider_model Widen the last dimension of the output of the pre_layer. Args pre_layer_id : The ID of a convolutional layer or dense layer. n_add : The number of dimensions to add. to_dense_deeper_model Insert a dense layer after the target layer. Args target_id : The ID of a dense layer. _conv_block_end_node Get the input node ID of the last layer in the block by layer ID. Args layer_id : the convolutional layer ID. Returns The input node ID of the last layer in the convolutional block. to_add_skip_model Add a weighted add skip connection from after start node to end node. Args start_id : The convolutional layer ID, after which to start the skip-connection. end_id : The convolutional layer ID, after which to end the skip-connection. to_concat_skip_model Add a weighted add concatenate connection from after start node to end node. Args start_id : The convolutional layer ID, after which to start the skip-connection. end_id : The convolutional layer ID, after which to end the skip-connection. produce_model Build a new model based on the current graph.","title":"graph"},{"location":"graph/#graph","text":"A class represent the neural architecture graph of a Keras model. Graph extracts the neural architecture graph from a Keras model. Each node in the graph is a intermediate tensor between layers. Each layer is an edge in the graph. Notably, multiple edges may refer to the same layer. (e.g. Add layer is adding two tensor into one tensor. So it is related to two edges.)","title":"Graph"},{"location":"graph/#attributes","text":"weighted : A boolean of whether the weights and biases in the neural network should be included in the graph. input_shape : Tuple of integers, does not include the batch axis. node_list : A list of integers, the indices of the list are the identifiers. layer_list : A list of stub layers, the indices of the list are the identifiers. node_to_id : A dict instance mapping from node integers to their identifiers. layer_to_id : A dict instance mapping from stub layers to their identifiers. layer_id_to_input_node_ids : A dict instance mapping from layer identifiers to their input nodes identifiers. adj_list : A two dimensional list. The adjacency list of the graph. The first dimension is identified by tensor identifiers. In each edge list, the elements are two-element tuples of (tensor identifier, layer identifier). reverse_adj_list : A reverse adjacent list in the same format as adj_list. operation_history : A list saving all the network morphism operations. vis : A dictionary of temporary storage for whether an local operation has been done during the network morphism.","title":"Attributes"},{"location":"graph/#n_nodes","text":"Return the number of nodes in the model.","title":"n_nodes"},{"location":"graph/#n_layers","text":"Return the number of layers in the model.","title":"n_layers"},{"location":"graph/#_add_node","text":"Add node to node list if it not in node list.","title":"_add_node"},{"location":"graph/#_add_edge","text":"Add edge to the graph.","title":"_add_edge"},{"location":"graph/#_redirect_edge","text":"Redirect the edge to a new node. Change the edge originally from u_id to v_id into an edge from u_id to new_v_id while keeping all other property of the edge the same.","title":"_redirect_edge"},{"location":"graph/#_replace_layer","text":"Replace the layer with a new layer.","title":"_replace_layer"},{"location":"graph/#topological_order","text":"Return the topological order of the node ids.","title":"topological_order"},{"location":"graph/#_search","text":"Search the graph for widening the layers.","title":"_search"},{"location":"graph/#args","text":"u : The starting node identifier. start_dim : The position to insert the additional dimensions. total_dim : The total number of dimensions the layer has before widening. n_add : The number of dimensions to add.","title":"Args"},{"location":"graph/#to_conv_deeper_model","text":"Insert a relu-conv-bn block after the target block.","title":"to_conv_deeper_model"},{"location":"graph/#args_1","text":"target_id : A convolutional layer ID. The new block should be inserted after the block. kernel_size : An integer. The kernel size of the new convolutional layer.","title":"Args"},{"location":"graph/#to_wider_model","text":"Widen the last dimension of the output of the pre_layer.","title":"to_wider_model"},{"location":"graph/#args_2","text":"pre_layer_id : The ID of a convolutional layer or dense layer. n_add : The number of dimensions to add.","title":"Args"},{"location":"graph/#to_dense_deeper_model","text":"Insert a dense layer after the target layer.","title":"to_dense_deeper_model"},{"location":"graph/#args_3","text":"target_id : The ID of a dense layer.","title":"Args"},{"location":"graph/#_conv_block_end_node","text":"Get the input node ID of the last layer in the block by layer ID.","title":"_conv_block_end_node"},{"location":"graph/#args_4","text":"layer_id : the convolutional layer ID.","title":"Args"},{"location":"graph/#returns","text":"The input node ID of the last layer in the convolutional block.","title":"Returns"},{"location":"graph/#to_add_skip_model","text":"Add a weighted add skip connection from after start node to end node.","title":"to_add_skip_model"},{"location":"graph/#args_5","text":"start_id : The convolutional layer ID, after which to start the skip-connection. end_id : The convolutional layer ID, after which to end the skip-connection.","title":"Args"},{"location":"graph/#to_concat_skip_model","text":"Add a weighted add concatenate connection from after start node to end node.","title":"to_concat_skip_model"},{"location":"graph/#args_6","text":"start_id : The convolutional layer ID, after which to start the skip-connection. end_id : The convolutional layer ID, after which to end the skip-connection.","title":"Args"},{"location":"graph/#produce_model","text":"Build a new model based on the current graph.","title":"produce_model"},{"location":"layer_transformer/","text":"","title":"Layer transformer"},{"location":"layers/","text":"","title":"Layers"},{"location":"net_combinator/","text":"","title":"Net combinator"},{"location":"net_transformer/","text":"","title":"Net transformer"},{"location":"preprocessor/","text":"OneHotEncoder A class that can format data This class provide ways to transform data's classification into vector Attributes data : the input data n_classes : the number of classification labels : the number of label label_to_vec : mapping from label to vector int_to_label : mapping from int to label init Init OneHotEncoder fit Create mapping from label to vector, and vector to label transform Get vector for every element in the data array inverse_transform Get label for every element in data","title":"preprocessor"},{"location":"preprocessor/#onehotencoder","text":"A class that can format data This class provide ways to transform data's classification into vector","title":"OneHotEncoder"},{"location":"preprocessor/#attributes","text":"data : the input data n_classes : the number of classification labels : the number of label label_to_vec : mapping from label to vector int_to_label : mapping from int to label","title":"Attributes"},{"location":"preprocessor/#init","text":"Init OneHotEncoder","title":"init"},{"location":"preprocessor/#fit","text":"Create mapping from label to vector, and vector to label","title":"fit"},{"location":"preprocessor/#transform","text":"Get vector for every element in the data array","title":"transform"},{"location":"preprocessor/#inverse_transform","text":"Get label for every element in data","title":"inverse_transform"},{"location":"search/","text":"BayesianSearcher Base class of all searcher class This class is the base class of all searcher class, every searcher class can override its search function to implements its strategy Attributes n_classes : number of classification input_shape : Arbitrary, although all dimensions in the input shaped must be fixed. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model. verbose : verbosity mode history : A list that stores the performance of model. Each element in it is a dictionary of 'model_id', 'loss', and 'accuracy'. path : A string. The path to the directory for saving the searcher. model_count : An integer. the total number of neural networks in the current searcher. descriptors : A dictionary of all the neural networks architectures searched. trainer_args : A dictionary. The params for the constructor of ModelTrainer. default_model_len : An integer. Number of convolutional layers in the initial architecture. default_model_width : An integer. The number of filters in each layer in the initial architecture. gpr : A GaussianProcessRegressor for bayesian optimization. search_tree : The data structure for storing all the searched architectures in tree structure. training_queue : A list of the generated architectures to be trained. x_queue : A list of trained architectures not updated to the gpr. y_queue : A list of trained architecture performances not updated to the gpr. beta : A float. The beta in the UCB acquisition function. t_min : A float. The minimum temperature during simulated annealing. init Args: n_classes: An integer, the number of classes. input_shape: A tuple. e.g. (28, 28, 1). path: A string. The path to the directory to save the searcher. verbose: A boolean. Whether to output the intermediate information to stdout. trainer_args: A dictionary. The params for the constructor of ModelTrainer. default_model_len: An integer. Number of convolutional layers in the initial architecture. default_model_width: An integer. The number of filters in each layer in the initial architecture. beta: A float. The beta in the UCB acquisition function. kernel_lambda: A float. The balance factor in the neural network kernel. t_min: A float. The minimum temperature during simulated annealing.","title":"search"},{"location":"search/#bayesiansearcher","text":"Base class of all searcher class This class is the base class of all searcher class, every searcher class can override its search function to implements its strategy","title":"BayesianSearcher"},{"location":"search/#attributes","text":"n_classes : number of classification input_shape : Arbitrary, although all dimensions in the input shaped must be fixed. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model. verbose : verbosity mode history : A list that stores the performance of model. Each element in it is a dictionary of 'model_id', 'loss', and 'accuracy'. path : A string. The path to the directory for saving the searcher. model_count : An integer. the total number of neural networks in the current searcher. descriptors : A dictionary of all the neural networks architectures searched. trainer_args : A dictionary. The params for the constructor of ModelTrainer. default_model_len : An integer. Number of convolutional layers in the initial architecture. default_model_width : An integer. The number of filters in each layer in the initial architecture. gpr : A GaussianProcessRegressor for bayesian optimization. search_tree : The data structure for storing all the searched architectures in tree structure. training_queue : A list of the generated architectures to be trained. x_queue : A list of trained architectures not updated to the gpr. y_queue : A list of trained architecture performances not updated to the gpr. beta : A float. The beta in the UCB acquisition function. t_min : A float. The minimum temperature during simulated annealing.","title":"Attributes"},{"location":"search/#init","text":"Args: n_classes: An integer, the number of classes. input_shape: A tuple. e.g. (28, 28, 1). path: A string. The path to the directory to save the searcher. verbose: A boolean. Whether to output the intermediate information to stdout. trainer_args: A dictionary. The params for the constructor of ModelTrainer. default_model_len: An integer. Number of convolutional layers in the initial architecture. default_model_width: An integer. The number of filters in each layer in the initial architecture. beta: A float. The beta in the UCB acquisition function. kernel_lambda: A float. The balance factor in the neural network kernel. t_min: A float. The minimum temperature during simulated annealing.","title":"init"},{"location":"start/","text":"Getting Started Installation The installation of Auto-Keras is the same as other python packages. Notably, currently we only support Python 3.6. Latest Stable Version ( pip installation): You can run the following pip installation command in your terminal to install the latest stable version. pip install autokeras Bleeding Edge Version (manual installation): If you want to install the latest development version. You need to download the code from the GitHub repo and run the following commands in the project directory. pip install -r requirements.txt python setup.py install Example We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs. Data with numpy array (.npy) format. If the images and the labels are already formatted into numpy arrays, you can from keras.datasets import mnist from autokeras.classifier import ImageClassifier if __name__ == '__main__': (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) clf = ImageClassifier(verbose=True) clf.fit(x_train, y_train, time_limit=12 * 60 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) y = clf.evaluate(x_test, y_test) print(y) In the example above, the images and the labels are already formatted into numpy arrays. What if your data are raw image files ( e.g. .jpg, .png, .bmp)? You can use our load_image_dataset function to load the images and there labels as follows. from autokeras.classifier import load_image_dataset x_train, y_train = load_image_dataset(csv_file_path=\"train/label.csv\", images_path=\"train\") print(x_train.shape) print(y_train.shape) x_test, y_test = load_image_dataset(csv_file_path=\"test/label.csv\", images_path=\"test\") print(x_test.shape) print(y_test.shape) The argument csv_file_path is the path to the CSV file containing the image file names and their corresponding labels. Here is an example of the csv file. File Name,Label 00000.jpg,5 00001.jpg,0 00002.jpg,4 00003.jpg,1 00004.jpg,9 00005.jpg,2 00006.jpg,1 ... The second argument images_path is the path to the directory containing all the images with those file names listed in the CSV file. The returned values x_train and y_train are the numpy arrays, which can be directly feed into the fit function of ImageClassifier .","title":"Getting Started"},{"location":"start/#getting-started","text":"","title":"Getting Started"},{"location":"start/#installation","text":"The installation of Auto-Keras is the same as other python packages. Notably, currently we only support Python 3.6.","title":"Installation"},{"location":"start/#latest-stable-version-pip-installation","text":"You can run the following pip installation command in your terminal to install the latest stable version. pip install autokeras","title":"Latest Stable Version (pip installation):"},{"location":"start/#bleeding-edge-version-manual-installation","text":"If you want to install the latest development version. You need to download the code from the GitHub repo and run the following commands in the project directory. pip install -r requirements.txt python setup.py install","title":"Bleeding Edge Version (manual installation):"},{"location":"start/#example","text":"We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs.","title":"Example"},{"location":"start/#data-with-numpy-array-npy-format","text":"If the images and the labels are already formatted into numpy arrays, you can from keras.datasets import mnist from autokeras.classifier import ImageClassifier if __name__ == '__main__': (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) clf = ImageClassifier(verbose=True) clf.fit(x_train, y_train, time_limit=12 * 60 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) y = clf.evaluate(x_test, y_test) print(y) In the example above, the images and the labels are already formatted into numpy arrays.","title":"Data with numpy array (.npy) format."},{"location":"start/#what-if-your-data-are-raw-image-files-eg-jpg-png-bmp","text":"You can use our load_image_dataset function to load the images and there labels as follows. from autokeras.classifier import load_image_dataset x_train, y_train = load_image_dataset(csv_file_path=\"train/label.csv\", images_path=\"train\") print(x_train.shape) print(y_train.shape) x_test, y_test = load_image_dataset(csv_file_path=\"test/label.csv\", images_path=\"test\") print(x_test.shape) print(y_test.shape) The argument csv_file_path is the path to the CSV file containing the image file names and their corresponding labels. Here is an example of the csv file. File Name,Label 00000.jpg,5 00001.jpg,0 00002.jpg,4 00003.jpg,1 00004.jpg,9 00005.jpg,2 00006.jpg,1 ... The second argument images_path is the path to the directory containing all the images with those file names listed in the CSV file. The returned values x_train and y_train are the numpy arrays, which can be directly feed into the fit function of ImageClassifier .","title":"What if your data are raw image files (e.g. .jpg, .png, .bmp)?"},{"location":"stub/","text":"","title":"Stub"},{"location":"utils/","text":"ensure_dir Create directory if it does not exist ensure_file_dir Create path if it does not exist ModelTrainer A class that is used to train model This class can train a model with dataset and will not stop until getting minimum loss Attributes model : the model that will be trained train_data : training data wrapped in batches. test_data : testing data wrapped in batches. verbose : verbosity mode init Init ModelTrainer with model, x_train, y_train, x_test, y_test, verbose train_model Train the model. Args max_iter_num : An integer. The maximum number of epochs to train the model. The training will stop when this number is reached. max_no_improvement_num : An integer. The maximum number of epochs when the loss value doesn't decrease. The training will stop when this number is reached. batch_size : An integer. The batch size during the training.","title":"utils"},{"location":"utils/#ensure_dir","text":"Create directory if it does not exist","title":"ensure_dir"},{"location":"utils/#ensure_file_dir","text":"Create path if it does not exist","title":"ensure_file_dir"},{"location":"utils/#modeltrainer","text":"A class that is used to train model This class can train a model with dataset and will not stop until getting minimum loss","title":"ModelTrainer"},{"location":"utils/#attributes","text":"model : the model that will be trained train_data : training data wrapped in batches. test_data : testing data wrapped in batches. verbose : verbosity mode","title":"Attributes"},{"location":"utils/#init","text":"Init ModelTrainer with model, x_train, y_train, x_test, y_test, verbose","title":"init"},{"location":"utils/#train_model","text":"Train the model.","title":"train_model"},{"location":"utils/#args","text":"max_iter_num : An integer. The maximum number of epochs to train the model. The training will stop when this number is reached. max_no_improvement_num : An integer. The maximum number of epochs when the loss value doesn't decrease. The training will stop when this number is reached. batch_size : An integer. The batch size during the training.","title":"Args"}]}